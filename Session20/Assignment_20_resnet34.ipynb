{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_20_resnet34.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pasumarthi/EVA/blob/master/Session20/Assignment_20_resnet34.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "up50HgbXHNcx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WEIGHTS_COLLECTION = [\n",
        "\n",
        "    # ResNet18\n",
        "    {\n",
        "        'model': 'resnet18',\n",
        "        'dataset': 'imagenet',\n",
        "        'classes': 1000,\n",
        "        'include_top': True,\n",
        "        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet18_imagenet_1000.h5',\n",
        "        'name': 'resnet18_imagenet_1000.h5',\n",
        "        'md5': '64da73012bb70e16c901316c201d9803',\n",
        "    },\n",
        "\n",
        "    {\n",
        "        'model': 'resnet18',\n",
        "        'dataset': 'imagenet',\n",
        "        'classes': 1000,\n",
        "        'include_top': False,\n",
        "        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet18_imagenet_1000_no_top.h5',\n",
        "        'name': 'resnet18_imagenet_1000.h5',\n",
        "        'md5': '318e3ac0cd98d51e917526c9f62f0b50',\n",
        "    },\n",
        "\n",
        "    # ResNet34\n",
        "    {\n",
        "        'model': 'resnet34',\n",
        "        'dataset': 'imagenet',\n",
        "        'classes': 1000,\n",
        "        'include_top': True,\n",
        "        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet34_imagenet_1000.h5',\n",
        "        'name': 'resnet34_imagenet_1000.h5',\n",
        "        'md5': '2ac8277412f65e5d047f255bcbd10383',\n",
        "    },\n",
        "\n",
        "    {\n",
        "        'model': 'resnet34',\n",
        "        'dataset': 'imagenet',\n",
        "        'classes': 1000,\n",
        "        'include_top': False,\n",
        "        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet34_imagenet_1000_no_top.h5',\n",
        "        'name': 'resnet34_imagenet_1000_no_top.h5',\n",
        "        'md5': '8caaa0ad39d927cb8ba5385bf945d582',\n",
        "    },\n",
        "\n",
        "    # ResNet50\n",
        "    {\n",
        "        'model': 'resnet50',\n",
        "        'dataset': 'imagenet',\n",
        "        'classes': 1000,\n",
        "        'include_top': True,\n",
        "        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet50_imagenet_1000.h5',\n",
        "        'name': 'resnet50_imagenet_1000.h5',\n",
        "        'md5': 'd0feba4fc650e68ac8c19166ee1ba87f',\n",
        "    },\n",
        "\n",
        "    {\n",
        "        'model': 'resnet50',\n",
        "        'dataset': 'imagenet',\n",
        "        'classes': 1000,\n",
        "        'include_top': False,\n",
        "        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet50_imagenet_1000_no_top.h5',\n",
        "        'name': 'resnet50_imagenet_1000_no_top.h5',\n",
        "        'md5': 'db3b217156506944570ac220086f09b6',\n",
        "    },\n",
        "\n",
        "]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx0CNrmxHOFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from . import get_submodules_from_kwargs\n",
        "\n",
        "__all__ = ['load_model_weights']\n",
        "\n",
        "\n",
        "def _find_weights(model_name, dataset, include_top):\n",
        "    w = list(filter(lambda x: x['model'] == model_name, WEIGHTS_COLLECTION))\n",
        "    w = list(filter(lambda x: x['dataset'] == dataset, w))\n",
        "    w = list(filter(lambda x: x['include_top'] == include_top, w))\n",
        "    return w\n",
        "\n",
        "\n",
        "def load_model_weights(model, model_name, dataset, classes, include_top, **kwargs):\n",
        "    _, _, _, keras_utils = get_submodules_from_kwargs(kwargs)\n",
        "\n",
        "    weights = _find_weights(model_name, dataset, include_top)\n",
        "\n",
        "    if weights:\n",
        "        weights = weights[0]\n",
        "\n",
        "        if include_top and weights['classes'] != classes:\n",
        "            raise ValueError('If using `weights` and `include_top`'\n",
        "                             ' as true, `classes` should be {}'.format(weights['classes']))\n",
        "\n",
        "        weights_path = get_file(\n",
        "            weights['name'],\n",
        "            weights['url'],\n",
        "            cache_subdir='models',\n",
        "            md5_hash=weights['md5']\n",
        "        )\n",
        "\n",
        "        model.load_weights(weights_path)\n",
        "\n",
        "    else:\n",
        "        raise ValueError('There is no weights for such configuration: ' +\n",
        "                         'model = {}, dataset = {}, '.format(model.name, dataset) +\n",
        "                         'classes = {}, include_top = {}.'.format(classes, include_top))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajphEaPLJ6v3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras_applications as ka\n",
        "# from .__version__ import __version__\n",
        "\n",
        "\n",
        "def get_submodules_from_kwargs(kwargs):\n",
        "    backend = kwargs.get('backend', ka._KERAS_BACKEND)\n",
        "    layers = kwargs.get('layers', ka._KERAS_LAYERS)\n",
        "    models = kwargs.get('models', ka._KERAS_MODELS)\n",
        "    utils = kwargs.get('utils', ka._KERAS_UTILS)\n",
        "    return backend, layers, models, utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpAGoLBWLm1V",
        "colab_type": "code",
        "outputId": "af40f439-4989-40fb-c80a-892069474109",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import GlobalMaxPooling2D, ZeroPadding2D, Add\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.merge import add\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras_applications.imagenet_utils import _obtain_input_shape\n",
        "from keras.utils import get_file"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcoLeEk_Gz65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import os\n",
        "import collections\n",
        "\n",
        "\n",
        "# from ._common_blocks import ChannelSE\n",
        "# from .. import get_submodules_from_kwargs\n",
        "# from ..weights import load_model_weights\n",
        "\n",
        "backend = None\n",
        "layers = None\n",
        "models = None\n",
        "keras_utils = None\n",
        "\n",
        "ModelParams = collections.namedtuple(\n",
        "    'ModelParams',\n",
        "    ['model_name', 'repetitions', 'residual_block', 'attention']\n",
        ")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "#   Helpers functions\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "def handle_block_names(stage, block):\n",
        "    name_base = 'stage{}_unit{}_'.format(stage + 1, block + 1)\n",
        "    conv_name = name_base + 'conv'\n",
        "    bn_name = name_base + 'bn'\n",
        "    relu_name = name_base + 'relu'\n",
        "    sc_name = name_base + 'sc'\n",
        "    return conv_name, bn_name, relu_name, sc_name\n",
        "\n",
        "\n",
        "def get_conv_params(**params):\n",
        "    default_conv_params = {\n",
        "        'kernel_initializer': 'he_uniform',\n",
        "        'use_bias': False,\n",
        "        'padding': 'valid',\n",
        "    }\n",
        "    default_conv_params.update(params)\n",
        "    return default_conv_params\n",
        "\n",
        "\n",
        "def get_bn_params(**params):\n",
        "    axis = 3 if K.image_data_format() == 'channels_last' else 1\n",
        "    default_bn_params = {\n",
        "        'axis': axis,\n",
        "        'momentum': 0.99,\n",
        "        'epsilon': 2e-5,\n",
        "        'center': True,\n",
        "        'scale': True,\n",
        "    }\n",
        "    default_bn_params.update(params)\n",
        "    return default_bn_params\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "#   Residual blocks\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "def residual_conv_block(filters, stage, block, strides=(1, 1), attention=None, cut='pre'):\n",
        "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
        "    # Arguments\n",
        "        input_tensor: input tensor\n",
        "        kernel_size: default 3, the kernel size of\n",
        "            middle conv layer at main path\n",
        "        filters: list of integers, the filters of 3 conv layer at main path\n",
        "        stage: integer, current stage label, used for generating layer names\n",
        "        block: 'a','b'..., current block label, used for generating layer names\n",
        "        cut: one of 'pre', 'post'. used to decide where skip connection is taken\n",
        "    # Returns\n",
        "        Output tensor for the block.\n",
        "    \"\"\"\n",
        "\n",
        "    def layer(input_tensor):\n",
        "\n",
        "        # get params and names of layers\n",
        "        conv_params = get_conv_params()\n",
        "        bn_params = get_bn_params()\n",
        "        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n",
        "\n",
        "        x = BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n",
        "        x = Activation('relu', name=relu_name + '1')(x)\n",
        "\n",
        "        # defining shortcut connection\n",
        "        if cut == 'pre':\n",
        "            shortcut = input_tensor\n",
        "        elif cut == 'post':\n",
        "            shortcut = Conv2D(filters, (1, 1), name=sc_name, strides=strides, **conv_params)(x)\n",
        "        else:\n",
        "            raise ValueError('Cut type not in [\"pre\", \"post\"]')\n",
        "\n",
        "        # continue with convolution layers\n",
        "        x = ZeroPadding2D(padding=(1, 1))(x)\n",
        "        x = Conv2D(filters, (3, 3), strides=strides, name=conv_name + '1', **conv_params)(x)\n",
        "#         x = Dropout(0.2)(x)\n",
        "\n",
        "        x = BatchNormalization(name=bn_name + '2', **bn_params)(x)\n",
        "        x = Activation('relu', name=relu_name + '2')(x)\n",
        "        x = ZeroPadding2D(padding=(1, 1))(x)\n",
        "        x = Conv2D(filters, (3, 3), name=conv_name + '2', **conv_params)(x)\n",
        "#         x = Dropout(0.2)(x)\n",
        "\n",
        "        # use attention block if defined\n",
        "        if attention is not None:\n",
        "            x = attention(x)\n",
        "\n",
        "        # add residual connection\n",
        "        x = Add()([x, shortcut])\n",
        "        return x\n",
        "\n",
        "    return layer\n",
        "\n",
        "\n",
        "def residual_bottleneck_block(filters, stage, block, strides=None, attention=None, cut='pre'):\n",
        "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
        "    # Arguments\n",
        "        input_tensor: input tensor\n",
        "        kernel_size: default 3, the kernel size of\n",
        "            middle conv layer at main path\n",
        "        filters: list of integers, the filters of 3 conv layer at main path\n",
        "        stage: integer, current stage label, used for generating layer names\n",
        "        block: 'a','b'..., current block label, used for generating layer names\n",
        "        cut: one of 'pre', 'post'. used to decide where skip connection is taken\n",
        "    # Returns\n",
        "        Output tensor for the block.\n",
        "    \"\"\"\n",
        "\n",
        "    def layer(input_tensor):\n",
        "\n",
        "        # get params and names of layers\n",
        "        conv_params = get_conv_params()\n",
        "        bn_params = get_bn_params()\n",
        "        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n",
        "\n",
        "        x = BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n",
        "        x = Activation('relu', name=relu_name + '1')(x)\n",
        "\n",
        "        # defining shortcut connection\n",
        "        if cut == 'pre':\n",
        "            shortcut = input_tensor\n",
        "        elif cut == 'post':\n",
        "            shortcut = Conv2D(filters * 4, (1, 1), name=sc_name, strides=strides, **conv_params)(x)\n",
        "        else:\n",
        "            raise ValueError('Cut type not in [\"pre\", \"post\"]')\n",
        "\n",
        "        # continue with convolution layers\n",
        "        x = Conv2D(filters, (1, 1), name=conv_name + '1', **conv_params)(x)\n",
        "\n",
        "        x = BatchNormalization(name=bn_name + '2', **bn_params)(x)\n",
        "        x = Activation('relu', name=relu_name + '2')(x)\n",
        "        x = ZeroPadding2D(padding=(1, 1))(x)\n",
        "        x = Conv2D(filters, (3, 3), strides=strides, name=conv_name + '2', **conv_params)(x)\n",
        "\n",
        "        x = BatchNormalization(name=bn_name + '3', **bn_params)(x)\n",
        "        x = Activation('relu', name=relu_name + '3')(x)\n",
        "        x = Conv2D(filters * 4, (1, 1), name=conv_name + '3', **conv_params)(x)\n",
        "\n",
        "        # use attention block if defined\n",
        "        if attention is not None:\n",
        "            x = attention(x)\n",
        "\n",
        "        # add residual connection\n",
        "        x = Add()([x, shortcut])\n",
        "\n",
        "        return x\n",
        "\n",
        "    return layer\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "#   Residual Model Builder\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def ResNet(model_params, input_shape=None, input_tensor=None, include_top=True,\n",
        "           classes=1000, weights='imagenet', **kwargs):\n",
        "\n",
        "    global backend, layers, models, keras_utils\n",
        "    backend, layers, models, keras_utils = get_submodules_from_kwargs(kwargs)\n",
        "\n",
        "    if input_tensor is None:\n",
        "        img_input = Input(shape=input_shape, name='data')\n",
        "    else:\n",
        "        if not backend.is_keras_tensor(input_tensor):\n",
        "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
        "        else:\n",
        "            img_input = input_tensor\n",
        "\n",
        "    # choose residual block type\n",
        "    ResidualBlock = model_params.residual_block\n",
        "    if model_params.attention:\n",
        "        Attention = model_params.attention(**kwargs)\n",
        "    else:\n",
        "        Attention = None\n",
        "\n",
        "    # get parameters for model layers\n",
        "    no_scale_bn_params = get_bn_params(scale=False)\n",
        "    bn_params = get_bn_params()\n",
        "    conv_params = get_conv_params()\n",
        "    init_filters = 64\n",
        "\n",
        "    # resnet bottom\n",
        "    x = BatchNormalization(name='bn_data', **no_scale_bn_params)(img_input)\n",
        "    x = ZeroPadding2D(padding=(3, 3))(x)\n",
        "    x = Conv2D(init_filters, (7, 7), strides=(1, 1), name='conv0', **conv_params)(x)\n",
        "#     x = Dropout(0.2)(x)\n",
        "    x = BatchNormalization(name='bn0', **bn_params)(x)\n",
        "    x = Activation('relu', name='relu0')(x)\n",
        "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
        "#     x = MaxPooling2D((3, 3), strides=(2, 2), padding='valid', name='pooling0')(x)\n",
        "\n",
        "    # resnet body\n",
        "    for stage, rep in enumerate(model_params.repetitions):\n",
        "        for block in range(rep):\n",
        "\n",
        "            filters = init_filters * (2 ** stage)\n",
        "\n",
        "            # first block of first stage without strides because we have maxpooling before\n",
        "            if block == 0 and stage == 0:\n",
        "                x = ResidualBlock(filters, stage, block, strides=(1, 1),\n",
        "                                  cut='post', attention=Attention)(x)\n",
        "\n",
        "            elif block == 0:\n",
        "                x = ResidualBlock(filters, stage, block, strides=(2, 2),\n",
        "                                  cut='post', attention=Attention)(x)\n",
        "\n",
        "            else:\n",
        "                x = ResidualBlock(filters, stage, block, strides=(1, 1),\n",
        "                                  cut='pre', attention=Attention)(x)\n",
        "\n",
        "    x = BatchNormalization(name='bn1', **bn_params)(x)\n",
        "    x = Activation('relu', name='relu1')(x)\n",
        "\n",
        "    # resnet top\n",
        "    if include_top:\n",
        "        x = GlobalAveragePooling2D(name='pool1')(x)\n",
        "        x = Dense(classes, name='fc1')(x)\n",
        "        x = Activation('softmax', name='softmax')(x)\n",
        "\n",
        "    # Ensure that the model takes into account any potential predecessors of `input_tensor`.\n",
        "    if input_tensor is not None:\n",
        "        inputs = keras_utils.get_source_inputs(input_tensor)\n",
        "    else:\n",
        "        inputs = img_input\n",
        "\n",
        "    # Create model.\n",
        "    model = Model(inputs, x)\n",
        "\n",
        "    if weights:\n",
        "        if type(weights) == str and os.path.exists(weights):\n",
        "            model.load_weights(weights)\n",
        "        else:\n",
        "            load_model_weights(model, model_params.model_name,\n",
        "                               weights, classes, include_top, **kwargs)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "#   Residual Models\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "MODELS_PARAMS = {\n",
        "    'resnet18': ModelParams('resnet18', (2, 2, 2, 2), residual_conv_block, None),\n",
        "    'resnet34': ModelParams('resnet34', (3, 4, 6, 3), residual_conv_block, None),\n",
        "    'resnet50': ModelParams('resnet50', (3, 4, 6, 3), residual_bottleneck_block, None),\n",
        "}\n",
        "\n",
        "\n",
        "def ResNet18(input_shape=None, input_tensor=None, weights=None, classes=1000, include_top=True, **kwargs):\n",
        "    return ResNet(\n",
        "        MODELS_PARAMS['resnet18'],\n",
        "        input_shape=input_shape,\n",
        "        input_tensor=input_tensor,\n",
        "        include_top=include_top,\n",
        "        classes=classes,\n",
        "        weights=weights,\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "\n",
        "def ResNet34(input_shape=None, input_tensor=None, weights=None, classes=1000, include_top=True, **kwargs):\n",
        "    return ResNet(\n",
        "        MODELS_PARAMS['resnet34'],\n",
        "        input_shape=input_shape,\n",
        "        input_tensor=input_tensor,\n",
        "        include_top=include_top,\n",
        "        classes=classes,\n",
        "        weights=weights,\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "\n",
        "def ResNet50(input_shape=None, input_tensor=None, weights=None, classes=1000, include_top=True, **kwargs):\n",
        "    return ResNet(\n",
        "        MODELS_PARAMS['resnet50'],\n",
        "        input_shape=input_shape,\n",
        "        input_tensor=input_tensor,\n",
        "        include_top=include_top,\n",
        "        classes=classes,\n",
        "        weights=weights,\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "\n",
        "def preprocess_input(x, **kwargs):\n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69aQqjcIddJP",
        "colab_type": "text"
      },
      "source": [
        "#### Changes made to the original architecture.\n",
        "Since the pretrined model is for imagenet which has input size of 224x224, and the fine tuned model is on cifar100 which has 32x32 size, incuded folloing minor changes that will reduce the effective global reciptive filed to suite 32x32:\n",
        "\n",
        "\n",
        "\n",
        "1.   The stride of first convoultion(7x7) is changed to 1 from its orignal value of 2 for the resnet model.\n",
        "2.   Max pooling layer is removed.\n",
        "\n",
        "These two steps will bring the input image size form 224 to 56 immediately before going into residual blocks. But in the case of 32x32 input image we do not need this reduction.\n",
        "\n",
        "This change will not effect the kernal weights and hence we are safe to use original weights file with out any changes.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I_CZfghgVLH",
        "colab_type": "text"
      },
      "source": [
        "In the following cell, we load imagenet pretrained model with out last fully connecred layer. This will allow us to have input size of our choice. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFhI-QbgglUq",
        "colab_type": "text"
      },
      "source": [
        "This is how we load imagenet weights to the Resnet architecture above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDjxp5C1JXCj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "outputId": "27dc19a0-3b48-4282-e9e0-52094c30d7cd"
      },
      "source": [
        "# model.summary()\n",
        "model = ResNet34(input_shape=(32,32,3), weights='imagenet', classes=100, include_top=False)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "Downloading data from https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet34_imagenet_1000_no_top.h5\n",
            "85524480/85521592 [==============================] - 23s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po8msVMOgxIK",
        "colab_type": "text"
      },
      "source": [
        "**Here we add a Fully connected layer to match the cifar100 dataset, with 100 neurons in the output layer, followed by a softmax.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AMubCdoNoqV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "8175945f-c92b-49dd-918e-889d58becfa4"
      },
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "\n",
        "x = model.output\n",
        "x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = keras.layers.Dropout(0.4)(x)\n",
        "predictions = keras.layers.Dense(100, activation= 'softmax')(x)\n",
        "model = Model(inputs = model.input, outputs = predictions)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDaXhzfLN6-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0QZYPBOEfDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.layers\n",
        "# for i,l in enumerate(model.layers):\n",
        "# #   print(l.trainable,l.name)\n",
        "#   if l.name == \"stage3_unit1_bn1\":\n",
        "#     break\n",
        "#   model.layers[i].trainable = False\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFKwYTWTGS4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for l in model.layers:\n",
        "#     print(l.name,\"--\", l.trainable)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0i_o2fUN9l7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "36053007-32d1-4b6b-ad0f-b400a64a5c1d"
      },
      "source": [
        "from keras.datasets import cifar100\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar100.load_data()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 11s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqCq5EA3OGuN",
        "colab_type": "code",
        "outputId": "cc14ce83-d1b6-4d6f-a3e1-667215c0de7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "trainX = x_train.astype('float32')\n",
        "testX = x_test.astype('float32')\n",
        "\n",
        "trainX /= 255.\n",
        "testX /= 255.\n",
        "\n",
        "nb_classes = 100\n",
        "Y_train = to_categorical(y_train, nb_classes)\n",
        "Y_test = to_categorical(y_test, nb_classes)\n",
        "Y_train.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb1Q4Nc0N_tl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51jfBgVZOHG7",
        "colab_type": "code",
        "outputId": "75f4d194-8568-49e3-b9a0-634537cafbc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.optimizers import SGD\n",
        "from keras.optimizers import adam\n",
        "\n",
        "\n",
        "random_erasing = True\n",
        "pixel_level = False\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "batch_size = 128\n",
        "# building model with too many classes inially for simpler transfer learning afterwards\n",
        "nb_classes = 100\n",
        "nb_epoch = 100\n",
        "# lr_list = np.linspace(0, 1, num=101);print(lr_list)\n",
        "# best_acc_lr_list = []\n",
        "\n",
        "# for lri in lr_list[1:]:\n",
        "#   print(lri)\n",
        "#   model = ResNet18(input_shape=(32,32,3), weights='imagenet', classes=100, include_top=False)\n",
        "#   x = model.output\n",
        "#   x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "#   x = keras.layers.Dropout(0.4)(x)\n",
        "#   predictions = keras.layers.Dense(100, activation= 'softmax')(x)\n",
        "#   model = Model(inputs = model.input, outputs = predictions)\n",
        "  \n",
        "datagen = ImageDataGenerator(featurewise_center=False,featurewise_std_normalization=False,\n",
        "                              preprocessing_function=get_random_eraser(p=0.5,v_l=0, v_h=1, pixel_level=pixel_level))\n",
        "\n",
        "# let's train the model using SGD + momentum (how original).\n",
        "sgd = SGD(lr=0.01, decay=1e-4, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "model_info = model.fit_generator(datagen.flow(trainX, Y_train, batch_size = batch_size),\n",
        "                                   samples_per_epoch = trainX.shape[0], nb_epoch = nb_epoch,\n",
        "                                   validation_data = (testX, Y_test), verbose=1)\n",
        "score = model.evaluate(testX, Y_test, verbose=1)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=(array([[[..., verbose=1, steps_per_epoch=390, epochs=100)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/100\n",
            "390/390 [==============================] - 395s 1s/step - loss: 3.1484 - acc: 0.2539 - val_loss: 2.4295 - val_acc: 0.4028\n",
            "Epoch 2/100\n",
            "390/390 [==============================] - 383s 982ms/step - loss: 1.6815 - acc: 0.5323 - val_loss: 1.4370 - val_acc: 0.5960\n",
            "Epoch 3/100\n",
            "390/390 [==============================] - 382s 979ms/step - loss: 1.1586 - acc: 0.6658 - val_loss: 1.2137 - val_acc: 0.6539\n",
            "Epoch 4/100\n",
            "390/390 [==============================] - 382s 979ms/step - loss: 0.8804 - acc: 0.7414 - val_loss: 1.1198 - val_acc: 0.6826\n",
            "Epoch 5/100\n",
            "390/390 [==============================] - 381s 977ms/step - loss: 0.6808 - acc: 0.7955 - val_loss: 1.0501 - val_acc: 0.7088\n",
            "Epoch 6/100\n",
            "390/390 [==============================] - 382s 980ms/step - loss: 0.5483 - acc: 0.8331 - val_loss: 1.0634 - val_acc: 0.7166\n",
            "Epoch 7/100\n",
            "390/390 [==============================] - 382s 980ms/step - loss: 0.4448 - acc: 0.8669 - val_loss: 1.1818 - val_acc: 0.7113\n",
            "Epoch 8/100\n",
            "390/390 [==============================] - 382s 980ms/step - loss: 0.3606 - acc: 0.8924 - val_loss: 1.1094 - val_acc: 0.7212\n",
            "Epoch 9/100\n",
            "390/390 [==============================] - 382s 980ms/step - loss: 0.3147 - acc: 0.9075 - val_loss: 1.2043 - val_acc: 0.7110\n",
            "Epoch 10/100\n",
            "390/390 [==============================] - 382s 980ms/step - loss: 0.2705 - acc: 0.9200 - val_loss: 1.0716 - val_acc: 0.7398\n",
            "Epoch 11/100\n",
            "390/390 [==============================] - 383s 981ms/step - loss: 0.2417 - acc: 0.9284 - val_loss: 1.1036 - val_acc: 0.7417\n",
            "Epoch 12/100\n",
            "390/390 [==============================] - 382s 980ms/step - loss: 0.2046 - acc: 0.9406 - val_loss: 1.1337 - val_acc: 0.7465\n",
            "Epoch 13/100\n",
            "390/390 [==============================] - 382s 979ms/step - loss: 0.1939 - acc: 0.9436 - val_loss: 1.1304 - val_acc: 0.7385\n",
            "Epoch 14/100\n",
            "390/390 [==============================] - 382s 979ms/step - loss: 0.1744 - acc: 0.9496 - val_loss: 1.1064 - val_acc: 0.7512\n",
            "Epoch 15/100\n",
            "390/390 [==============================] - 382s 980ms/step - loss: 0.1625 - acc: 0.9533 - val_loss: 1.1355 - val_acc: 0.7505\n",
            "Epoch 16/100\n",
            "390/390 [==============================] - 382s 980ms/step - loss: 0.1476 - acc: 0.9569 - val_loss: 1.1068 - val_acc: 0.7498\n",
            "Epoch 17/100\n",
            "390/390 [==============================] - 382s 980ms/step - loss: 0.1392 - acc: 0.9603 - val_loss: 1.0956 - val_acc: 0.7561\n",
            "Epoch 18/100\n",
            "390/390 [==============================] - 382s 979ms/step - loss: 0.1274 - acc: 0.9642 - val_loss: 1.1351 - val_acc: 0.7572\n",
            "Epoch 19/100\n",
            "390/390 [==============================] - 382s 978ms/step - loss: 0.1253 - acc: 0.9637 - val_loss: 1.0766 - val_acc: 0.7615\n",
            "Epoch 20/100\n",
            "390/390 [==============================] - 382s 978ms/step - loss: 0.1135 - acc: 0.9689 - val_loss: 1.1347 - val_acc: 0.7571\n",
            "Epoch 21/100\n",
            "390/390 [==============================] - 381s 977ms/step - loss: 0.1061 - acc: 0.9696 - val_loss: 1.1381 - val_acc: 0.7560\n",
            "Epoch 22/100\n",
            "390/390 [==============================] - 381s 977ms/step - loss: 0.1038 - acc: 0.9703 - val_loss: 1.0902 - val_acc: 0.7695\n",
            "Epoch 23/100\n",
            "390/390 [==============================] - 382s 979ms/step - loss: 0.1033 - acc: 0.9708 - val_loss: 1.1626 - val_acc: 0.7534\n",
            "Epoch 24/100\n",
            "390/390 [==============================] - 381s 978ms/step - loss: 0.0969 - acc: 0.9724 - val_loss: 1.1130 - val_acc: 0.7578\n",
            "Epoch 25/100\n",
            "390/390 [==============================] - 382s 979ms/step - loss: 0.0934 - acc: 0.9737 - val_loss: 1.0878 - val_acc: 0.7669\n",
            "Epoch 26/100\n",
            "390/390 [==============================] - 382s 978ms/step - loss: 0.0877 - acc: 0.9758 - val_loss: 1.1050 - val_acc: 0.7635\n",
            "Epoch 27/100\n",
            "390/390 [==============================] - 381s 976ms/step - loss: 0.0829 - acc: 0.9760 - val_loss: 1.0437 - val_acc: 0.7725\n",
            "Epoch 28/100\n",
            "390/390 [==============================] - 381s 977ms/step - loss: 0.0814 - acc: 0.9769 - val_loss: 1.1135 - val_acc: 0.7676\n",
            "Epoch 29/100\n",
            "390/390 [==============================] - 381s 978ms/step - loss: 0.0793 - acc: 0.9774 - val_loss: 1.0909 - val_acc: 0.7703\n",
            "Epoch 30/100\n",
            "390/390 [==============================] - 382s 979ms/step - loss: 0.0706 - acc: 0.9800 - val_loss: 1.1018 - val_acc: 0.7677\n",
            "Epoch 31/100\n",
            "390/390 [==============================] - 382s 980ms/step - loss: 0.0714 - acc: 0.9798 - val_loss: 1.0719 - val_acc: 0.7716\n",
            "Epoch 32/100\n",
            "390/390 [==============================] - 382s 979ms/step - loss: 0.0703 - acc: 0.9808 - val_loss: 1.1001 - val_acc: 0.7710\n",
            "Epoch 33/100\n",
            "390/390 [==============================] - 381s 978ms/step - loss: 0.0698 - acc: 0.9800 - val_loss: 1.1315 - val_acc: 0.7696\n",
            "Epoch 34/100\n",
            "390/390 [==============================] - 381s 978ms/step - loss: 0.0686 - acc: 0.9804 - val_loss: 1.0575 - val_acc: 0.7774\n",
            "Epoch 35/100\n",
            "390/390 [==============================] - 381s 976ms/step - loss: 0.0630 - acc: 0.9819 - val_loss: 1.1025 - val_acc: 0.7725\n",
            "Epoch 36/100\n",
            "390/390 [==============================] - 381s 976ms/step - loss: 0.0640 - acc: 0.9814 - val_loss: 1.0773 - val_acc: 0.7721\n",
            "Epoch 37/100\n",
            "390/390 [==============================] - 381s 977ms/step - loss: 0.0571 - acc: 0.9834 - val_loss: 1.1054 - val_acc: 0.7729\n",
            "Epoch 38/100\n",
            "390/390 [==============================] - 381s 977ms/step - loss: 0.0585 - acc: 0.9834 - val_loss: 1.1267 - val_acc: 0.7688\n",
            "Epoch 39/100\n",
            "390/390 [==============================] - 381s 977ms/step - loss: 0.0552 - acc: 0.9845 - val_loss: 1.0925 - val_acc: 0.7732\n",
            "Epoch 40/100\n",
            "390/390 [==============================] - 382s 979ms/step - loss: 0.0535 - acc: 0.9847 - val_loss: 1.0963 - val_acc: 0.7748\n",
            "Epoch 41/100\n",
            "390/390 [==============================] - 382s 981ms/step - loss: 0.0527 - acc: 0.9849 - val_loss: 1.1047 - val_acc: 0.7760\n",
            "Epoch 42/100\n",
            "390/390 [==============================] - 383s 983ms/step - loss: 0.0502 - acc: 0.9858 - val_loss: 1.0874 - val_acc: 0.7758\n",
            "Epoch 43/100\n",
            "390/390 [==============================] - 384s 983ms/step - loss: 0.0487 - acc: 0.9859 - val_loss: 1.0988 - val_acc: 0.7747\n",
            "Epoch 44/100\n",
            "390/390 [==============================] - 383s 981ms/step - loss: 0.0521 - acc: 0.9858 - val_loss: 1.0827 - val_acc: 0.7770\n",
            "Epoch 45/100\n",
            "390/390 [==============================] - 382s 981ms/step - loss: 0.0495 - acc: 0.9856 - val_loss: 1.1314 - val_acc: 0.7715\n",
            "Epoch 46/100\n",
            "390/390 [==============================] - 382s 978ms/step - loss: 0.0461 - acc: 0.9870 - val_loss: 1.0706 - val_acc: 0.7805\n",
            "Epoch 47/100\n",
            "390/390 [==============================] - 381s 977ms/step - loss: 0.0443 - acc: 0.9872 - val_loss: 1.1024 - val_acc: 0.7757\n",
            "Epoch 48/100\n",
            "390/390 [==============================] - 381s 976ms/step - loss: 0.0441 - acc: 0.9875 - val_loss: 1.1023 - val_acc: 0.7773\n",
            "Epoch 49/100\n",
            "390/390 [==============================] - 381s 976ms/step - loss: 0.0385 - acc: 0.9888 - val_loss: 1.1188 - val_acc: 0.7741\n",
            "Epoch 50/100\n",
            "390/390 [==============================] - 380s 975ms/step - loss: 0.0393 - acc: 0.9884 - val_loss: 1.0941 - val_acc: 0.7793\n",
            "Epoch 51/100\n",
            "390/390 [==============================] - 381s 976ms/step - loss: 0.0410 - acc: 0.9884 - val_loss: 1.0790 - val_acc: 0.7748\n",
            "Epoch 52/100\n",
            "390/390 [==============================] - 380s 975ms/step - loss: 0.0400 - acc: 0.9883 - val_loss: 1.0977 - val_acc: 0.7795\n",
            "Epoch 53/100\n",
            "390/390 [==============================] - 381s 977ms/step - loss: 0.0391 - acc: 0.9887 - val_loss: 1.0919 - val_acc: 0.7800\n",
            "Epoch 54/100\n",
            "390/390 [==============================] - 380s 975ms/step - loss: 0.0379 - acc: 0.9886 - val_loss: 1.1184 - val_acc: 0.7789\n",
            "Epoch 55/100\n",
            "390/390 [==============================] - 380s 974ms/step - loss: 0.0375 - acc: 0.9888 - val_loss: 1.0783 - val_acc: 0.7848\n",
            "Epoch 56/100\n",
            "390/390 [==============================] - 380s 975ms/step - loss: 0.0413 - acc: 0.9884 - val_loss: 1.0844 - val_acc: 0.7853\n",
            "Epoch 57/100\n",
            "390/390 [==============================] - 380s 975ms/step - loss: 0.0356 - acc: 0.9896 - val_loss: 1.0862 - val_acc: 0.7804\n",
            "Epoch 58/100\n",
            "390/390 [==============================] - 380s 974ms/step - loss: 0.0336 - acc: 0.9902 - val_loss: 1.0531 - val_acc: 0.7868\n",
            "Epoch 59/100\n",
            "390/390 [==============================] - 380s 973ms/step - loss: 0.0340 - acc: 0.9903 - val_loss: 1.0837 - val_acc: 0.7824\n",
            "Epoch 60/100\n",
            "390/390 [==============================] - 380s 975ms/step - loss: 0.0355 - acc: 0.9901 - val_loss: 1.0752 - val_acc: 0.7837\n",
            "Epoch 61/100\n",
            "390/390 [==============================] - 380s 975ms/step - loss: 0.0326 - acc: 0.9907 - val_loss: 1.1040 - val_acc: 0.7823\n",
            "Epoch 62/100\n",
            "390/390 [==============================] - 380s 974ms/step - loss: 0.0338 - acc: 0.9900 - val_loss: 1.0973 - val_acc: 0.7834\n",
            "Epoch 63/100\n",
            "390/390 [==============================] - 380s 975ms/step - loss: 0.0317 - acc: 0.9910 - val_loss: 1.1064 - val_acc: 0.7830\n",
            "Epoch 64/100\n",
            "390/390 [==============================] - 380s 975ms/step - loss: 0.0318 - acc: 0.9909 - val_loss: 1.1101 - val_acc: 0.7807\n",
            "Epoch 65/100\n",
            "390/390 [==============================] - 380s 974ms/step - loss: 0.0323 - acc: 0.9906 - val_loss: 1.0980 - val_acc: 0.7817\n",
            "Epoch 66/100\n",
            "390/390 [==============================] - 387s 992ms/step - loss: 0.0336 - acc: 0.9906 - val_loss: 1.1095 - val_acc: 0.7828\n",
            "Epoch 67/100\n",
            "390/390 [==============================] - 381s 977ms/step - loss: 0.0309 - acc: 0.9912 - val_loss: 1.0895 - val_acc: 0.7830\n",
            "Epoch 68/100\n",
            "390/390 [==============================] - 382s 979ms/step - loss: 0.0295 - acc: 0.9917 - val_loss: 1.1112 - val_acc: 0.7802\n",
            "Epoch 69/100\n",
            "390/390 [==============================] - 382s 979ms/step - loss: 0.0310 - acc: 0.9911 - val_loss: 1.0936 - val_acc: 0.7822\n",
            "Epoch 70/100\n",
            "390/390 [==============================] - 382s 980ms/step - loss: 0.0279 - acc: 0.9921 - val_loss: 1.0900 - val_acc: 0.7841\n",
            "Epoch 71/100\n",
            "390/390 [==============================] - 382s 980ms/step - loss: 0.0285 - acc: 0.9917 - val_loss: 1.1071 - val_acc: 0.7839\n",
            "Epoch 72/100\n",
            "390/390 [==============================] - 382s 980ms/step - loss: 0.0281 - acc: 0.9920 - val_loss: 1.0971 - val_acc: 0.7841\n",
            "Epoch 73/100\n",
            "390/390 [==============================] - 382s 980ms/step - loss: 0.0290 - acc: 0.9913 - val_loss: 1.0921 - val_acc: 0.7796\n",
            "Epoch 74/100\n",
            "390/390 [==============================] - 382s 980ms/step - loss: 0.0270 - acc: 0.9922 - val_loss: 1.0603 - val_acc: 0.7865\n",
            "Epoch 75/100\n",
            "390/390 [==============================] - 382s 979ms/step - loss: 0.0279 - acc: 0.9917 - val_loss: 1.0910 - val_acc: 0.7829\n",
            "Epoch 76/100\n",
            "390/390 [==============================] - 382s 979ms/step - loss: 0.0256 - acc: 0.9925 - val_loss: 1.0792 - val_acc: 0.7827\n",
            "Epoch 77/100\n",
            "390/390 [==============================] - 382s 980ms/step - loss: 0.0271 - acc: 0.9923 - val_loss: 1.1057 - val_acc: 0.7773\n",
            "Epoch 78/100\n",
            "390/390 [==============================] - 382s 979ms/step - loss: 0.0279 - acc: 0.9919 - val_loss: 1.1022 - val_acc: 0.7804\n",
            "Epoch 79/100\n",
            "390/390 [==============================] - 383s 981ms/step - loss: 0.0266 - acc: 0.9923 - val_loss: 1.0840 - val_acc: 0.7807\n",
            "Epoch 80/100\n",
            "390/390 [==============================] - 382s 981ms/step - loss: 0.0245 - acc: 0.9931 - val_loss: 1.0883 - val_acc: 0.7817\n",
            "Epoch 81/100\n",
            "390/390 [==============================] - 383s 981ms/step - loss: 0.0269 - acc: 0.9922 - val_loss: 1.0856 - val_acc: 0.7817\n",
            "Epoch 82/100\n",
            "390/390 [==============================] - 384s 984ms/step - loss: 0.0237 - acc: 0.9933 - val_loss: 1.0817 - val_acc: 0.7825\n",
            "Epoch 83/100\n",
            "390/390 [==============================] - 384s 984ms/step - loss: 0.0258 - acc: 0.9925 - val_loss: 1.0794 - val_acc: 0.7831\n",
            "Epoch 84/100\n",
            "390/390 [==============================] - 384s 984ms/step - loss: 0.0224 - acc: 0.9931 - val_loss: 1.0984 - val_acc: 0.7796\n",
            "Epoch 85/100\n",
            "390/390 [==============================] - 384s 983ms/step - loss: 0.0258 - acc: 0.9930 - val_loss: 1.0856 - val_acc: 0.7811\n",
            "Epoch 86/100\n",
            "390/390 [==============================] - 384s 984ms/step - loss: 0.0224 - acc: 0.9935 - val_loss: 1.0898 - val_acc: 0.7825\n",
            "Epoch 87/100\n",
            "390/390 [==============================] - 383s 983ms/step - loss: 0.0206 - acc: 0.9944 - val_loss: 1.0794 - val_acc: 0.7817\n",
            "Epoch 88/100\n",
            "390/390 [==============================] - 384s 984ms/step - loss: 0.0222 - acc: 0.9934 - val_loss: 1.0924 - val_acc: 0.7834\n",
            "Epoch 89/100\n",
            "390/390 [==============================] - 384s 984ms/step - loss: 0.0202 - acc: 0.9942 - val_loss: 1.0867 - val_acc: 0.7848\n",
            "Epoch 90/100\n",
            "390/390 [==============================] - 384s 984ms/step - loss: 0.0211 - acc: 0.9939 - val_loss: 1.0842 - val_acc: 0.7848\n",
            "Epoch 91/100\n",
            "390/390 [==============================] - 384s 984ms/step - loss: 0.0192 - acc: 0.9945 - val_loss: 1.0913 - val_acc: 0.7851\n",
            "Epoch 92/100\n",
            "390/390 [==============================] - 384s 984ms/step - loss: 0.0212 - acc: 0.9939 - val_loss: 1.0995 - val_acc: 0.7816\n",
            "Epoch 93/100\n",
            "390/390 [==============================] - 384s 985ms/step - loss: 0.0204 - acc: 0.9942 - val_loss: 1.1034 - val_acc: 0.7808\n",
            "Epoch 94/100\n",
            "390/390 [==============================] - 384s 985ms/step - loss: 0.0207 - acc: 0.9939 - val_loss: 1.1003 - val_acc: 0.7830\n",
            "Epoch 95/100\n",
            "390/390 [==============================] - 382s 980ms/step - loss: 0.0194 - acc: 0.9945 - val_loss: 1.0900 - val_acc: 0.7828\n",
            "Epoch 96/100\n",
            "390/390 [==============================] - 382s 978ms/step - loss: 0.0192 - acc: 0.9943 - val_loss: 1.0895 - val_acc: 0.7826\n",
            "Epoch 97/100\n",
            "390/390 [==============================] - 382s 979ms/step - loss: 0.0201 - acc: 0.9943 - val_loss: 1.0964 - val_acc: 0.7815\n",
            "Epoch 98/100\n",
            "390/390 [==============================] - 381s 978ms/step - loss: 0.0206 - acc: 0.9944 - val_loss: 1.0775 - val_acc: 0.7828\n",
            "Epoch 99/100\n",
            "390/390 [==============================] - 381s 978ms/step - loss: 0.0213 - acc: 0.9941 - val_loss: 1.0841 - val_acc: 0.7855\n",
            "Epoch 100/100\n",
            "390/390 [==============================] - 381s 977ms/step - loss: 0.0192 - acc: 0.9945 - val_loss: 1.1011 - val_acc: 0.7843\n",
            "10000/10000 [==============================] - 32s 3ms/step\n",
            "Test score: 1.101129850578308\n",
            "Test accuracy: 0.7843\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7OMxFc2e3Q_",
        "colab_type": "text"
      },
      "source": [
        "#### Accuracy of 78.43% in 100 epochs on cifar100\n",
        "\n",
        "Since we removed two strided conv layer and a maxpooling layer, the resnet34 model takes much longer time to train.\n",
        "\n",
        "\n",
        "Achieved a val acc of 77.93% in 50 epochs. \n",
        "\n",
        "With some efficient hyperparameter turning, and also setsome layers to learn other than last layer , it should be possible to achieve higher accuracy much quickly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7hu_2POOKpP",
        "colab_type": "code",
        "outputId": "293f5c74-929b-4188-9e24-9380bab9aa48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Nov 14 00:04:03 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.50       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P0    57W / 149W |   8420MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ItaINMBD2wo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}