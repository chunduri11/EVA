{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_7b_v3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pasumarthi/EVA/blob/master/Session7/Assignment_7b_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkwXnw9OfHZl",
        "colab_type": "code",
        "outputId": "3a5b9009-bbf5-4635-bf12-189dfcdb3b84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras import backend as K\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "% matplotlib inline\n",
        "np.random.seed(2017) \n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, Conv2D, SeparableConv2D\n",
        "from keras.layers import Activation, Flatten, Dense, Dropout, Concatenate, GlobalAveragePooling2D, Input,Lambda\n",
        "from keras.layers.advanced_activations import ReLU, Softmax\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "import tensorflow as tf\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3F_WyKh5ibq",
        "colab_type": "code",
        "outputId": "805a4e06-8302-4744-9a3b-7e48680ac9a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "  (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "  y_train = keras.utils.to_categorical(y_train, 10)\n",
        "  y_test  = keras.utils.to_categorical(y_test, 10)\n",
        "  x_train = x_train.astype('float32')\n",
        "  x_test  = x_test.astype('float32')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 9s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14HyBUXdfS6G",
        "colab_type": "code",
        "outputId": "3b214279-f2f2-4cce-f2ef-349d27c06184",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        }
      },
      "source": [
        "class_names = ['airplane','automobile','bird','cat','deer',\n",
        "               'dog','frog','horse','ship','truck']\n",
        "fig = plt.figure(figsize=(8,3))\n",
        "for i in range(10):\n",
        "    ax = fig.add_subplot(2, 5, 1 + i, xticks=[], yticks=[])\n",
        "    idx = np.where(train_labels[:]==i)[0]\n",
        "    features_idx = train_features[idx,::]\n",
        "    img_num = np.random.randint(features_idx.shape[0])\n",
        "    im = features_idx[img_num]\n",
        "    ax.set_title(class_names[i])\n",
        "    plt.imshow(im)\n",
        "plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-eaf2a53d0903>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mfeatures_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mimg_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_labels' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGIAAABfCAYAAAATQRJ9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAATtJREFUeJzt3TFKBEEQQNFp8QhjbN//LDOHMNY7\ntIFg6qyw7Effiyso+FBpjbXWxuM9PXoBvggRIUSEEBFCRAgRIUSEEBFCRDzfMrzv+5pz3mmVv+k8\nz4+11stPczeFmHNux3H8fqt/aIzxdmXOaYoQIkKICCEihIgQIkKICCEihIgQIkKICCEihIgQIkKI\nCCEihIgQIkKICCEihIgQIkKICCEihIgQIkKICCEihIgQIkKICCEihIgQIkKICCEihIgQIkKICCEi\nhIgQIkKICCEihIgQIkKICCEihIgQIkKICCEihIgQIkKICCEihIgQIkKICCEihIgQIkKICCEihIgQ\nIkKICCEihIgQIkKICCEixlrr+vAY79u2XXpwx7fXK58ZbwrB/ThNEUJECBEhRIQQEUJECBEhRIQQ\nEZ8zsBpfR9RJ4wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmfsk76-fadV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_model_history(model_history):\n",
        "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
        "    # summarize history for accuracy\n",
        "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
        "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
        "    axs[0].legend(['train', 'val'], loc='best')\n",
        "    # summarize history for loss\n",
        "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
        "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
        "    axs[1].legend(['train', 'val'], loc='best')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJMT4rjgfdZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(test_x, test_y, model):\n",
        "    result = model.predict(test_x)\n",
        "    predicted_class = np.argmax(result, axis=1)\n",
        "    true_class = np.argmax(test_y, axis=1)\n",
        "    num_correct = np.sum(predicted_class == true_class) \n",
        "    accuracy = float(num_correct)/result.shape[0]\n",
        "    return (accuracy * 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idvkqoLGbww5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def space_to_depth_bs2(x):\n",
        "  return tf.space_to_depth(x, block_size=2)\n",
        "\n",
        "def space_to_depth_bs4(x):\n",
        "  return tf.space_to_depth(x, block_size=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yynQe-hNsa04",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "da7f2cb7-7610-4385-a4d3-82900cf868d7"
      },
      "source": [
        "## MAIN BLOCK\n",
        "from keras.layers import Activation\n",
        "\n",
        "\n",
        "x1 = Input(shape=(32,32,3))\n",
        "\n",
        "\n",
        "x2=SeparableConv2D(16, (5,5), padding='same',  use_bias=False)(x1)#32\n",
        "x2 = ReLU()(x2)\n",
        "x2 = BatchNormalization()(x2)\n",
        "x2 = Dropout(0.2)(x2)\n",
        "\n",
        "x3 = Conv2D(32, (5,5), padding='same', use_bias=False)(x2)#32\n",
        "x3 = ReLU()(x3)\n",
        "x3 = BatchNormalization()(x3)\n",
        "x3 = Dropout(0.2)(x3)\n",
        "\n",
        "x4 = Conv2D(64, (5,5), padding='same', use_bias=False)(x3)#32\n",
        "x4 = ReLU()(x4)\n",
        "x4 = BatchNormalization()(x4)\n",
        "x4 = Dropout(0.2)(x4)\n",
        "\n",
        "\n",
        "x4_x5= Concatenate(axis=-1)([x4,x2])\n",
        "x5=SeparableConv2D(64, (5,5), padding='same',  use_bias=False)(x4_x5)#32\n",
        "x5 = ReLU()(x2)\n",
        "x5 = BatchNormalization()(x5)\n",
        "x5 = Dropout(0.5)(x5)\n",
        "\n",
        "##block2\n",
        "x2_block2 = Lambda(space_to_depth_bs2)(x2)\n",
        "x3_block2 = Lambda(space_to_depth_bs2)(x3)\n",
        "x4_block2 = Lambda(space_to_depth_bs2)(x4)\n",
        "x5_block2 = Lambda(space_to_depth_bs2)(x5)\n",
        "\n",
        "\n",
        "\n",
        "x5 = Conv2D(32, (1,1), padding='same', use_bias=False)(x5)#32\n",
        "x2_x5= Concatenate(axis=-1)([x2,x5])\n",
        "#x2_x5 = Conv2D(32, (1,1), padding='same', use_bias=False)(x2_x5)#32\n",
        "x6 =  MaxPooling2D(pool_size=(2, 2))(x2_x5) #16\n",
        "\n",
        "x7=SeparableConv2D(32, (3,3), padding='same',  use_bias=False)(x6)#16\n",
        "x7 = ReLU()(x7)\n",
        "x7 = BatchNormalization()(x7)\n",
        "x7 = Dropout(0.5)(x7)\n",
        "\n",
        "x2_x5_x7=Concatenate(axis=-1)([x2_block2,x5_block2,x7])\n",
        "x8= Conv2D(64, (5,5), padding='same', use_bias=False)(x2_x5_x7)#16\n",
        "x8 = ReLU()(x8)\n",
        "x8 = BatchNormalization()(x8)\n",
        "x8 = Dropout(0.3)(x8)\n",
        "\n",
        "x8_x4_x5_x7=Concatenate(axis=-1)([x8,x4_block2,x5_block2,x7])\n",
        "x9=SeparableConv2D(64, (3,3), padding='same',  use_bias=False)(x8_x4_x5_x7)#16\n",
        "x9 = ReLU()(x9)\n",
        "x9 = BatchNormalization()(x9)\n",
        "\n",
        "x9_x8_x7_x5_x4_x2=Concatenate(axis=-1)([x2_block2,x4_block2,x5_block2,x7,x8,x9])\n",
        "x10=SeparableConv2D(64, (5,5), padding='same',  use_bias=False)(x9_x8_x7_x5_x4_x2)#\n",
        "x10 = ReLU()(x10)\n",
        "x10 = BatchNormalization()(x10)\n",
        "x10 = Dropout(0.5)(x10)\n",
        "\n",
        "\n",
        "x10 = Conv2D(16, (1,1), padding='same', use_bias=False)(x10)\n",
        "x10_x9_x7_x5_x2=Concatenate(axis=-1)([x2_block2,x5_block2,x7,x9,x10])\n",
        "\n",
        "#x10_x9_x7_x5_x2 = Conv2D(16, (1,1), padding='same', use_bias=False)(x10_x9_x7_x5_x2)\n",
        "x11=MaxPooling2D(pool_size=(2, 2))(x10_x9_x7_x5_x2)#8\n",
        "## block3\n",
        "x2_block3 = Lambda(space_to_depth_bs4)(x2)\n",
        "x3_block3 = Lambda(space_to_depth_bs4)(x3)\n",
        "x4_block3 = Lambda(space_to_depth_bs4)(x4)\n",
        "x5_block3 = Lambda(space_to_depth_bs4)(x5)\n",
        "\n",
        "x7_block3 = Lambda(space_to_depth_bs2)(x7)\n",
        "x8_block3 = Lambda(space_to_depth_bs2)(x8)\n",
        "x9_block3 = Lambda(space_to_depth_bs2)(x9)\n",
        "x10_block3 = Lambda(space_to_depth_bs2)(x10)\n",
        "\n",
        "\n",
        "x11_x8=Concatenate(axis=-1)([x8_block3,x11])\n",
        "x12=Conv2D(32, (5,5), padding='same', use_bias=False)(x11_x8)#8\n",
        "x12 = ReLU()(x12)\n",
        "x12 = BatchNormalization()(x12)\n",
        "x12 = Dropout(0.3)(x12)\n",
        "\n",
        "x12_x9_x5_x3=Concatenate(axis=-1)([x3_block3,x5_block3,x9_block3,x12])\n",
        "x13=SeparableConv2D(32, (5,5), padding='same',  use_bias=False)(x12_x9_x5_x3)#\n",
        "x13 = ReLU()(x13)\n",
        "x13 = BatchNormalization()(x13)\n",
        "x13 = Dropout(0.5)(x13)\n",
        "\n",
        "x13_x12_x7_x4_x3=Concatenate(axis=-1)([x3_block3,x4_block3,x7_block3,x12,x13])\n",
        "x14=Conv2D(64, (5,5), padding='same', use_bias=False)(x13_x12_x7_x4_x3)#8\n",
        "x14 = ReLU()(x14)\n",
        "x14 = BatchNormalization()(x14)\n",
        "x14 = Dropout(0.5)(x14)\n",
        "\n",
        "x14_x13_x9_x7_x5_x4_x2=Concatenate(axis=-1)([x2_block3,x4_block3,x5_block3,x7_block3,x9_block3,x13,x14])\n",
        "x15=SeparableConv2D(128, (5,5), padding='same',  use_bias=False)(x14_x13_x9_x7_x5_x4_x2)#8\n",
        "x15 = ReLU()(x15)\n",
        "\n",
        "\n",
        "\n",
        "x15_x13_x9_x5=Concatenate(axis=-1)([x5_block3,x9_block3,x13,x15])\n",
        "\n",
        "\n",
        "\"\"\"reduce_ch = Conv2D(10, (1,1), use_bias=False)(x15_x13_x9_x5)#8\n",
        "avg_pool = GlobalAveragePooling2D()(reduce_ch)\n",
        "output = Softmax()(avg_pool)\n",
        "\"\"\"\n",
        "reduce_ch = Conv2D(10, (8,8), use_bias=False)(x15_x13_x9_x5)#8\n",
        "x16 = Flatten()(reduce_ch)\n",
        "output = Softmax()(x16)\n",
        "\n",
        "\n",
        "model = Model(inputs=[x1], outputs=[output])\n",
        "\n",
        "#print(datetime.datetime.now())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0615 23:06:10.516012 140514709612416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0615 23:06:10.551638 140514709612416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0615 23:06:10.559758 140514709612416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0615 23:06:10.604554 140514709612416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0615 23:06:10.605465 140514709612416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0615 23:06:13.143123 140514709612416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0615 23:06:13.216996 140514709612416 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0615 23:06:13.610433 140514709612416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPC8cIOecjRm",
        "colab_type": "code",
        "outputId": "942a8e29-ee45-4580-ea0d-13fed8212301",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3091
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_1 (SeparableCo (None, 32, 32, 16)   123         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_1 (ReLU)                  (None, 32, 32, 16)   0           separable_conv2d_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          re_lu_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_4 (ReLU)                  (None, 32, 32, 16)   0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          re_lu_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 32)   512         dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 48)   0           dropout_1[0][0]                  \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 48)   0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_3 (SeparableCo (None, 16, 16, 32)   1968        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 32)   12800       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_5 (ReLU)                  (None, 16, 16, 32)   0           separable_conv2d_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_2 (ReLU)                  (None, 32, 32, 32)   0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 16, 16, 32)   128         re_lu_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 32)   128         re_lu_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 16, 16, 64)   0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, 16, 16, 64)   0           dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 16, 16, 32)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 32, 32, 32)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 16, 16, 160)  0           lambda_1[0][0]                   \n",
            "                                                                 lambda_4[0][0]                   \n",
            "                                                                 dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 64)   51200       dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 16, 16, 64)   256000      concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_3 (ReLU)                  (None, 32, 32, 64)   0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_6 (ReLU)                  (None, 16, 16, 64)   0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 64)   256         re_lu_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 16, 16, 64)   256         re_lu_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 32, 32, 64)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 16, 16, 64)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 16, 16, 256)  0           dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 16, 16, 416)  0           dropout_6[0][0]                  \n",
            "                                                                 lambda_3[0][0]                   \n",
            "                                                                 lambda_4[0][0]                   \n",
            "                                                                 dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_4 (SeparableCo (None, 16, 16, 64)   30368       concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_7 (ReLU)                  (None, 16, 16, 64)   0           separable_conv2d_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 16, 16, 64)   256         re_lu_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 16, 16, 544)  0           lambda_1[0][0]                   \n",
            "                                                                 lambda_3[0][0]                   \n",
            "                                                                 lambda_4[0][0]                   \n",
            "                                                                 dropout_5[0][0]                  \n",
            "                                                                 dropout_6[0][0]                  \n",
            "                                                                 batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_5 (SeparableCo (None, 16, 16, 64)   48416       concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_8 (ReLU)                  (None, 16, 16, 64)   0           separable_conv2d_5[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 16, 64)   256         re_lu_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 16, 16, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 16, 16, 16)   1024        dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 16, 16, 240)  0           lambda_1[0][0]                   \n",
            "                                                                 lambda_4[0][0]                   \n",
            "                                                                 dropout_5[0][0]                  \n",
            "                                                                 batch_normalization_7[0][0]      \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_10 (Lambda)              (None, 8, 8, 256)    0           dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 240)    0           concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 8, 8, 496)    0           lambda_10[0][0]                  \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 8, 8, 32)     396800      concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_9 (ReLU)                  (None, 8, 8, 32)     0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 8, 8, 32)     128         re_lu_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_8 (Lambda)               (None, 8, 8, 512)    0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_11 (Lambda)              (None, 8, 8, 256)    0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "lambda_6 (Lambda)               (None, 8, 8, 512)    0           dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 8, 8, 32)     0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 8, 8, 1312)   0           lambda_6[0][0]                   \n",
            "                                                                 lambda_8[0][0]                   \n",
            "                                                                 lambda_11[0][0]                  \n",
            "                                                                 dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_6 (SeparableCo (None, 8, 8, 32)     74784       concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_10 (ReLU)                 (None, 8, 8, 32)     0           separable_conv2d_6[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 8, 8, 32)     128         re_lu_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 8, 8, 32)     0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_7 (Lambda)               (None, 8, 8, 1024)   0           dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_9 (Lambda)               (None, 8, 8, 128)    0           dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 8, 8, 1728)   0           lambda_6[0][0]                   \n",
            "                                                                 lambda_7[0][0]                   \n",
            "                                                                 lambda_9[0][0]                   \n",
            "                                                                 dropout_8[0][0]                  \n",
            "                                                                 dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 8, 8, 64)     2764800     concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_11 (ReLU)                 (None, 8, 8, 64)     0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 8, 8, 64)     256         re_lu_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, 8, 8, 256)    0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 8, 8, 64)     0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 8, 8, 2272)   0           lambda_5[0][0]                   \n",
            "                                                                 lambda_7[0][0]                   \n",
            "                                                                 lambda_8[0][0]                   \n",
            "                                                                 lambda_9[0][0]                   \n",
            "                                                                 lambda_11[0][0]                  \n",
            "                                                                 dropout_9[0][0]                  \n",
            "                                                                 dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_7 (SeparableCo (None, 8, 8, 128)    347616      concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "re_lu_12 (ReLU)                 (None, 8, 8, 128)    0           separable_conv2d_7[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 8, 8, 928)    0           lambda_8[0][0]                   \n",
            "                                                                 lambda_11[0][0]                  \n",
            "                                                                 dropout_9[0][0]                  \n",
            "                                                                 re_lu_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 1, 1, 10)     593920      concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 10)           0           conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "softmax_1 (Softmax)             (None, 10)           0           flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 4,582,251\n",
            "Trainable params: 4,581,291\n",
            "Non-trainable params: 960\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVJq3WPOsClX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "def scheduler(epoch):\n",
        "  if (epoch <= 3 ):\n",
        "   return 0.003\n",
        "  elif(epoch <=6):\n",
        "   return 0.002\n",
        "  elif(epoch <=9):\n",
        "   return 0.001\n",
        "  elif(epoch <=13):\n",
        "   return 0.0005 \n",
        "  else: \n",
        "   return round(0.001 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "734ba5ae-293b-4ab5-e9e5-bdf0ed6ecb12",
        "id": "T8-gF-r0r0QE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7402
        }
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#datagen   = ImageDataGenerator(horizontal_flip=True,width_shift_range=0.125,height_shift_range=0.125)\n",
        "datagen = ImageDataGenerator(zoom_range=0.0,horizontal_flip=False)\n",
        "datagen.fit(x_train)\n",
        "train_iterator = datagen.flow(x_train, y_train, batch_size=128)\n",
        "test_iterator = datagen.flow(x_test, y_test, batch_size=128)\n",
        "\n",
        "lr1=LearningRateScheduler(scheduler, verbose=1)\n",
        "start = time.time()\n",
        "model_info= model.fit_generator(train_iterator, steps_per_epoch=len(train_iterator), epochs=100, verbose=1, validation_data=test_iterator,\n",
        "                   validation_steps=len(test_iterator),callbacks=[lr1])\n",
        "end = time.time()\n",
        "print (\"Model took %0.2f seconds to train\" %(end - start))\n",
        "# plot model history\n",
        "plot_model_history(model_info)\n",
        "# compute test accuracy\n",
        "print (\"Accuracy on test data is: %0.2f\"%accuracy(test_features, test_labels, model))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0615 23:07:52.120324 140514709612416 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0615 23:07:52.682009 140514709612416 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "391/391 [==============================] - 62s 159ms/step - loss: 2.9178 - acc: 0.3722 - val_loss: 1.5204 - val_acc: 0.4877\n",
            "Epoch 2/100\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.003.\n",
            "391/391 [==============================] - 54s 138ms/step - loss: 1.6503 - acc: 0.4987 - val_loss: 1.5863 - val_acc: 0.4818\n",
            "Epoch 3/100\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.003.\n",
            "391/391 [==============================] - 54s 138ms/step - loss: 1.3682 - acc: 0.5695 - val_loss: 1.2332 - val_acc: 0.5783\n",
            "Epoch 4/100\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.003.\n",
            "391/391 [==============================] - 54s 138ms/step - loss: 1.2570 - acc: 0.6069 - val_loss: 1.2347 - val_acc: 0.5769\n",
            "Epoch 5/100\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.002.\n",
            "391/391 [==============================] - 54s 138ms/step - loss: 1.2589 - acc: 0.6195 - val_loss: 1.2835 - val_acc: 0.5650\n",
            "Epoch 6/100\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.002.\n",
            "391/391 [==============================] - 54s 138ms/step - loss: 1.0974 - acc: 0.6582 - val_loss: 1.0925 - val_acc: 0.6282\n",
            "Epoch 7/100\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.002.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 1.0574 - acc: 0.6710 - val_loss: 1.0218 - val_acc: 0.6592\n",
            "Epoch 8/100\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.001.\n",
            "391/391 [==============================] - 54s 138ms/step - loss: 0.9569 - acc: 0.7070 - val_loss: 0.9765 - val_acc: 0.6750\n",
            "Epoch 9/100\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.001.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.9331 - acc: 0.7159 - val_loss: 1.0075 - val_acc: 0.6704\n",
            "Epoch 10/100\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.001.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.8929 - acc: 0.7230 - val_loss: 1.0116 - val_acc: 0.6707\n",
            "Epoch 11/100\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0005.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.8326 - acc: 0.7452 - val_loss: 0.9707 - val_acc: 0.6831\n",
            "Epoch 12/100\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.0005.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.8112 - acc: 0.7504 - val_loss: 1.0060 - val_acc: 0.6720\n",
            "Epoch 13/100\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0005.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.8048 - acc: 0.7518 - val_loss: 1.0180 - val_acc: 0.6716\n",
            "Epoch 14/100\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005.\n",
            "391/391 [==============================] - 54s 138ms/step - loss: 0.7718 - acc: 0.7629 - val_loss: 0.9815 - val_acc: 0.6864\n",
            "Epoch 15/100\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0001829491.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.7404 - acc: 0.7746 - val_loss: 0.9864 - val_acc: 0.6841\n",
            "Epoch 16/100\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0001728608.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.7207 - acc: 0.7765 - val_loss: 1.0076 - val_acc: 0.6807\n",
            "Epoch 17/100\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000163827.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.7145 - acc: 0.7811 - val_loss: 1.0231 - val_acc: 0.6770\n",
            "Epoch 18/100\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0001556905.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.7013 - acc: 0.7861 - val_loss: 1.0416 - val_acc: 0.6754\n",
            "Epoch 19/100\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0001483239.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.7014 - acc: 0.7862 - val_loss: 1.0072 - val_acc: 0.6878\n",
            "Epoch 20/100\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000141623.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.6906 - acc: 0.7910 - val_loss: 1.0343 - val_acc: 0.6802\n",
            "Epoch 21/100\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0001355014.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.6857 - acc: 0.7894 - val_loss: 1.0273 - val_acc: 0.6836\n",
            "Epoch 22/100\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.000129887.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.6787 - acc: 0.7924 - val_loss: 1.0166 - val_acc: 0.6869\n",
            "Epoch 23/100\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0001247194.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.6727 - acc: 0.7940 - val_loss: 1.0226 - val_acc: 0.6847\n",
            "Epoch 24/100\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0001199472.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.6732 - acc: 0.7932 - val_loss: 1.0230 - val_acc: 0.6864\n",
            "Epoch 25/100\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0001155268.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.6675 - acc: 0.7968 - val_loss: 1.0250 - val_acc: 0.6847\n",
            "Epoch 26/100\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0001114206.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.6566 - acc: 0.7969 - val_loss: 1.0246 - val_acc: 0.6852\n",
            "Epoch 27/100\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0001075963.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.6468 - acc: 0.8021 - val_loss: 1.0635 - val_acc: 0.6806\n",
            "Epoch 28/100\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0001040258.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.6479 - acc: 0.8025 - val_loss: 1.0430 - val_acc: 0.6835\n",
            "Epoch 29/100\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.0001006847.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.6414 - acc: 0.8033 - val_loss: 1.0548 - val_acc: 0.6850\n",
            "Epoch 30/100\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 9.75515e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.6409 - acc: 0.8031 - val_loss: 1.0552 - val_acc: 0.6825\n",
            "Epoch 31/100\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 9.46074e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.6374 - acc: 0.8052 - val_loss: 1.0658 - val_acc: 0.6816\n",
            "Epoch 32/100\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 9.18358e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.6288 - acc: 0.8077 - val_loss: 1.0523 - val_acc: 0.6833\n",
            "Epoch 33/100\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 8.9222e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.6288 - acc: 0.8087 - val_loss: 1.0504 - val_acc: 0.6865\n",
            "Epoch 34/100\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 8.67528e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.6303 - acc: 0.8085 - val_loss: 1.0487 - val_acc: 0.6848\n",
            "Epoch 35/100\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 8.44167e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.6173 - acc: 0.8105 - val_loss: 1.0547 - val_acc: 0.6839\n",
            "Epoch 36/100\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 8.2203e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.6133 - acc: 0.8119 - val_loss: 1.0637 - val_acc: 0.6843\n",
            "Epoch 37/100\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 8.01025e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.6117 - acc: 0.8125 - val_loss: 1.0625 - val_acc: 0.6846\n",
            "Epoch 38/100\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 7.81067e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.6138 - acc: 0.8115 - val_loss: 1.0873 - val_acc: 0.6804\n",
            "Epoch 39/100\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 7.62079e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.6144 - acc: 0.8111 - val_loss: 1.0584 - val_acc: 0.6844\n",
            "Epoch 40/100\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 7.43992e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.6060 - acc: 0.8144 - val_loss: 1.0699 - val_acc: 0.6848\n",
            "Epoch 41/100\n",
            "\n",
            "Epoch 00041: LearningRateScheduler setting learning rate to 7.26744e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5934 - acc: 0.8172 - val_loss: 1.0696 - val_acc: 0.6846\n",
            "Epoch 42/100\n",
            "\n",
            "Epoch 00042: LearningRateScheduler setting learning rate to 7.10278e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.6021 - acc: 0.8174 - val_loss: 1.0688 - val_acc: 0.6866\n",
            "Epoch 43/100\n",
            "\n",
            "Epoch 00043: LearningRateScheduler setting learning rate to 6.94541e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5901 - acc: 0.8181 - val_loss: 1.0744 - val_acc: 0.6862\n",
            "Epoch 44/100\n",
            "\n",
            "Epoch 00044: LearningRateScheduler setting learning rate to 6.79486e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5872 - acc: 0.8189 - val_loss: 1.0815 - val_acc: 0.6837\n",
            "Epoch 45/100\n",
            "\n",
            "Epoch 00045: LearningRateScheduler setting learning rate to 6.6507e-05.\n",
            "391/391 [==============================] - 53s 136ms/step - loss: 0.5869 - acc: 0.8195 - val_loss: 1.0784 - val_acc: 0.6864\n",
            "Epoch 46/100\n",
            "\n",
            "Epoch 00046: LearningRateScheduler setting learning rate to 6.51254e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5862 - acc: 0.8196 - val_loss: 1.0909 - val_acc: 0.6839\n",
            "Epoch 47/100\n",
            "\n",
            "Epoch 00047: LearningRateScheduler setting learning rate to 6.37999e-05.\n",
            "391/391 [==============================] - 53s 137ms/step - loss: 0.5805 - acc: 0.8201 - val_loss: 1.0901 - val_acc: 0.6840\n",
            "Epoch 48/100\n",
            "\n",
            "Epoch 00048: LearningRateScheduler setting learning rate to 6.25274e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5765 - acc: 0.8224 - val_loss: 1.0866 - val_acc: 0.6849\n",
            "Epoch 49/100\n",
            "\n",
            "Epoch 00049: LearningRateScheduler setting learning rate to 6.13046e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5728 - acc: 0.8239 - val_loss: 1.1059 - val_acc: 0.6826\n",
            "Epoch 50/100\n",
            "\n",
            "Epoch 00050: LearningRateScheduler setting learning rate to 6.01287e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5744 - acc: 0.8241 - val_loss: 1.1004 - val_acc: 0.6847\n",
            "Epoch 51/100\n",
            "\n",
            "Epoch 00051: LearningRateScheduler setting learning rate to 5.89971e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5725 - acc: 0.8239 - val_loss: 1.0973 - val_acc: 0.6842\n",
            "Epoch 52/100\n",
            "\n",
            "Epoch 00052: LearningRateScheduler setting learning rate to 5.79072e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5667 - acc: 0.8251 - val_loss: 1.0994 - val_acc: 0.6854\n",
            "Epoch 53/100\n",
            "\n",
            "Epoch 00053: LearningRateScheduler setting learning rate to 5.68569e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5613 - acc: 0.8256 - val_loss: 1.1114 - val_acc: 0.6828\n",
            "Epoch 54/100\n",
            "\n",
            "Epoch 00054: LearningRateScheduler setting learning rate to 5.58441e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5656 - acc: 0.8248 - val_loss: 1.0978 - val_acc: 0.6853\n",
            "Epoch 55/100\n",
            "\n",
            "Epoch 00055: LearningRateScheduler setting learning rate to 5.48667e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5638 - acc: 0.8296 - val_loss: 1.1231 - val_acc: 0.6786\n",
            "Epoch 56/100\n",
            "\n",
            "Epoch 00056: LearningRateScheduler setting learning rate to 5.39229e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5646 - acc: 0.8262 - val_loss: 1.1022 - val_acc: 0.6848\n",
            "Epoch 57/100\n",
            "\n",
            "Epoch 00057: LearningRateScheduler setting learning rate to 5.3011e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5604 - acc: 0.8283 - val_loss: 1.1209 - val_acc: 0.6794\n",
            "Epoch 58/100\n",
            "\n",
            "Epoch 00058: LearningRateScheduler setting learning rate to 5.21295e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5498 - acc: 0.8300 - val_loss: 1.1208 - val_acc: 0.6814\n",
            "Epoch 59/100\n",
            "\n",
            "Epoch 00059: LearningRateScheduler setting learning rate to 5.12768e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5490 - acc: 0.8317 - val_loss: 1.1119 - val_acc: 0.6826\n",
            "Epoch 60/100\n",
            "\n",
            "Epoch 00060: LearningRateScheduler setting learning rate to 5.04515e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5462 - acc: 0.8319 - val_loss: 1.1155 - val_acc: 0.6835\n",
            "Epoch 61/100\n",
            "\n",
            "Epoch 00061: LearningRateScheduler setting learning rate to 4.96524e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5479 - acc: 0.8293 - val_loss: 1.1174 - val_acc: 0.6833\n",
            "Epoch 62/100\n",
            "\n",
            "Epoch 00062: LearningRateScheduler setting learning rate to 4.88782e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5471 - acc: 0.8311 - val_loss: 1.1116 - val_acc: 0.6837\n",
            "Epoch 63/100\n",
            "\n",
            "Epoch 00063: LearningRateScheduler setting learning rate to 4.81278e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5420 - acc: 0.8322 - val_loss: 1.1109 - val_acc: 0.6835\n",
            "Epoch 64/100\n",
            "\n",
            "Epoch 00064: LearningRateScheduler setting learning rate to 4.74001e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5368 - acc: 0.8341 - val_loss: 1.1206 - val_acc: 0.6845\n",
            "Epoch 65/100\n",
            "\n",
            "Epoch 00065: LearningRateScheduler setting learning rate to 4.66941e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5430 - acc: 0.8315 - val_loss: 1.1266 - val_acc: 0.6822\n",
            "Epoch 66/100\n",
            "\n",
            "Epoch 00066: LearningRateScheduler setting learning rate to 4.60087e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5411 - acc: 0.8323 - val_loss: 1.1211 - val_acc: 0.6808\n",
            "Epoch 67/100\n",
            "\n",
            "Epoch 00067: LearningRateScheduler setting learning rate to 4.53432e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5360 - acc: 0.8336 - val_loss: 1.1296 - val_acc: 0.6831\n",
            "Epoch 68/100\n",
            "\n",
            "Epoch 00068: LearningRateScheduler setting learning rate to 4.46967e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5325 - acc: 0.8335 - val_loss: 1.1224 - val_acc: 0.6839\n",
            "Epoch 69/100\n",
            "\n",
            "Epoch 00069: LearningRateScheduler setting learning rate to 4.40684e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5286 - acc: 0.8367 - val_loss: 1.1375 - val_acc: 0.6811\n",
            "Epoch 70/100\n",
            "\n",
            "Epoch 00070: LearningRateScheduler setting learning rate to 4.34575e-05.\n",
            "391/391 [==============================] - 54s 138ms/step - loss: 0.5267 - acc: 0.8362 - val_loss: 1.1362 - val_acc: 0.6831\n",
            "Epoch 71/100\n",
            "\n",
            "Epoch 00071: LearningRateScheduler setting learning rate to 4.28633e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5158 - acc: 0.8388 - val_loss: 1.1424 - val_acc: 0.6816\n",
            "Epoch 72/100\n",
            "\n",
            "Epoch 00072: LearningRateScheduler setting learning rate to 4.22851e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5269 - acc: 0.8392 - val_loss: 1.1406 - val_acc: 0.6820\n",
            "Epoch 73/100\n",
            "\n",
            "Epoch 00073: LearningRateScheduler setting learning rate to 4.17223e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5228 - acc: 0.8401 - val_loss: 1.1503 - val_acc: 0.6802\n",
            "Epoch 74/100\n",
            "\n",
            "Epoch 00074: LearningRateScheduler setting learning rate to 4.11743e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5222 - acc: 0.8394 - val_loss: 1.1451 - val_acc: 0.6827\n",
            "Epoch 75/100\n",
            "\n",
            "Epoch 00075: LearningRateScheduler setting learning rate to 4.06405e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5240 - acc: 0.8386 - val_loss: 1.1403 - val_acc: 0.6851\n",
            "Epoch 76/100\n",
            "\n",
            "Epoch 00076: LearningRateScheduler setting learning rate to 4.01204e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5178 - acc: 0.8382 - val_loss: 1.1650 - val_acc: 0.6812\n",
            "Epoch 77/100\n",
            "\n",
            "Epoch 00077: LearningRateScheduler setting learning rate to 3.96134e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5189 - acc: 0.8394 - val_loss: 1.1617 - val_acc: 0.6817\n",
            "Epoch 78/100\n",
            "\n",
            "Epoch 00078: LearningRateScheduler setting learning rate to 3.9119e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5105 - acc: 0.8417 - val_loss: 1.1466 - val_acc: 0.6840\n",
            "Epoch 79/100\n",
            "\n",
            "Epoch 00079: LearningRateScheduler setting learning rate to 3.86369e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5088 - acc: 0.8455 - val_loss: 1.1629 - val_acc: 0.6828\n",
            "Epoch 80/100\n",
            "\n",
            "Epoch 00080: LearningRateScheduler setting learning rate to 3.81665e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5030 - acc: 0.8452 - val_loss: 1.1823 - val_acc: 0.6797\n",
            "Epoch 81/100\n",
            "\n",
            "Epoch 00081: LearningRateScheduler setting learning rate to 3.77074e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5038 - acc: 0.8444 - val_loss: 1.1631 - val_acc: 0.6829\n",
            "Epoch 82/100\n",
            "\n",
            "Epoch 00082: LearningRateScheduler setting learning rate to 3.72592e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.4969 - acc: 0.8441 - val_loss: 1.1687 - val_acc: 0.6795\n",
            "Epoch 83/100\n",
            "\n",
            "Epoch 00083: LearningRateScheduler setting learning rate to 3.68216e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.4963 - acc: 0.8450 - val_loss: 1.1702 - val_acc: 0.6785\n",
            "Epoch 84/100\n",
            "\n",
            "Epoch 00084: LearningRateScheduler setting learning rate to 3.63941e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.4954 - acc: 0.8468 - val_loss: 1.1674 - val_acc: 0.6807\n",
            "Epoch 85/100\n",
            "\n",
            "Epoch 00085: LearningRateScheduler setting learning rate to 3.59764e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.4986 - acc: 0.8460 - val_loss: 1.1784 - val_acc: 0.6786\n",
            "Epoch 86/100\n",
            "\n",
            "Epoch 00086: LearningRateScheduler setting learning rate to 3.55682e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5011 - acc: 0.8452 - val_loss: 1.1719 - val_acc: 0.6835\n",
            "Epoch 87/100\n",
            "\n",
            "Epoch 00087: LearningRateScheduler setting learning rate to 3.51692e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5012 - acc: 0.8452 - val_loss: 1.1791 - val_acc: 0.6819\n",
            "Epoch 88/100\n",
            "\n",
            "Epoch 00088: LearningRateScheduler setting learning rate to 3.4779e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.4972 - acc: 0.8475 - val_loss: 1.1711 - val_acc: 0.6827\n",
            "Epoch 89/100\n",
            "\n",
            "Epoch 00089: LearningRateScheduler setting learning rate to 3.43974e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.4951 - acc: 0.8464 - val_loss: 1.1788 - val_acc: 0.6833\n",
            "Epoch 90/100\n",
            "\n",
            "Epoch 00090: LearningRateScheduler setting learning rate to 3.4024e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.4974 - acc: 0.8444 - val_loss: 1.1716 - val_acc: 0.6814\n",
            "Epoch 91/100\n",
            "\n",
            "Epoch 00091: LearningRateScheduler setting learning rate to 3.36587e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.5028 - acc: 0.8469 - val_loss: 1.1771 - val_acc: 0.6834\n",
            "Epoch 92/100\n",
            "\n",
            "Epoch 00092: LearningRateScheduler setting learning rate to 3.33011e-05.\n",
            "391/391 [==============================] - 53s 137ms/step - loss: 0.4907 - acc: 0.8496 - val_loss: 1.1680 - val_acc: 0.6830\n",
            "Epoch 93/100\n",
            "\n",
            "Epoch 00093: LearningRateScheduler setting learning rate to 3.29511e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.4873 - acc: 0.8492 - val_loss: 1.1819 - val_acc: 0.6809\n",
            "Epoch 94/100\n",
            "\n",
            "Epoch 00094: LearningRateScheduler setting learning rate to 3.26083e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.4905 - acc: 0.8478 - val_loss: 1.1742 - val_acc: 0.6829\n",
            "Epoch 95/100\n",
            "\n",
            "Epoch 00095: LearningRateScheduler setting learning rate to 3.22726e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.4815 - acc: 0.8494 - val_loss: 1.1723 - val_acc: 0.6828\n",
            "Epoch 96/100\n",
            "\n",
            "Epoch 00096: LearningRateScheduler setting learning rate to 3.19438e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.4877 - acc: 0.8494 - val_loss: 1.1831 - val_acc: 0.6822\n",
            "Epoch 97/100\n",
            "\n",
            "Epoch 00097: LearningRateScheduler setting learning rate to 3.16216e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.4845 - acc: 0.8510 - val_loss: 1.1806 - val_acc: 0.6818\n",
            "Epoch 98/100\n",
            "\n",
            "Epoch 00098: LearningRateScheduler setting learning rate to 3.13058e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.4833 - acc: 0.8505 - val_loss: 1.1879 - val_acc: 0.6819\n",
            "Epoch 99/100\n",
            "\n",
            "Epoch 00099: LearningRateScheduler setting learning rate to 3.09962e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.4791 - acc: 0.8506 - val_loss: 1.1887 - val_acc: 0.6808\n",
            "Epoch 100/100\n",
            "\n",
            "Epoch 00100: LearningRateScheduler setting learning rate to 3.06927e-05.\n",
            "391/391 [==============================] - 54s 137ms/step - loss: 0.4754 - acc: 0.8525 - val_loss: 1.1877 - val_acc: 0.6830\n",
            "Model took 5375.80 seconds to train\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3sAAAFNCAYAAAC5cXZ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-7e69926af450>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mplot_model_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# compute test accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy on test data is: %0.2f\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_features' is not defined"
          ]
        }
      ]
    }
  ]
}